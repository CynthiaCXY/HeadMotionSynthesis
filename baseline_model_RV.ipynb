{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "from hms.layers import AffineLayer, SoftmaxLayer, SigmoidLayer, ReluLayer, LeakyReluLayer, DropoutLayer, BatchNormalizationLayer\n",
    "from hms.errors import CrossEntropyError, CrossEntropySoftmaxError, SumOfSquaredDiffsError, L1Error\n",
    "from hms.models import SingleLayerModel, MultipleLayerModel\n",
    "from hms.initialisers import UniformInit, GlorotUniformInit, ConstantInit\n",
    "from hms.learning_rules import GradientDescentLearningRule, AdamLearningRule\n",
    "from hms.data_providers import HMSDataProvider, HMS300dDataProvider\n",
    "from hms.optimisers import Optimiser\n",
    "from hms.penalties import L1Penalty, L2Penalty\n",
    "import seaborn as sns;\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seed a random number generator\n",
    "seed = 6102016 \n",
    "rng = np.random.RandomState(seed)\n",
    "\n",
    "# Set up a logger object to print info about the training run to stdout\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.handlers = [logging.StreamHandler()]\n",
    "\n",
    "# Create data provider objects for the MNIST data set\n",
    "train_data = HMS300dDataProvider('train', 'extro', 'Wiki', rng=rng)\n",
    "valid_data = HMS300dDataProvider('validation', 'extro', 'Wiki', rng=rng)\n",
    "input_dim, output_dim = 300, 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Penalty coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weights_penalty_2 = [L2Penalty(1e-1), L2Penalty(1e-2), L2Penalty(1e-4), L2Penalty(1e-6), L2Penalty(1e-8)]\n",
    "prediction_L2 = []\n",
    "motion_data = np.load('data/Wiki/validation_extro.npz')['targets']\n",
    "\n",
    "batch_size = 100  # number of data points in a batch\n",
    "init_scale = 0.01  # scale for random parameter initialisation\n",
    "learning_rate = 0.001  # learning rate for gradient descent\n",
    "num_epochs = 30  # number of training epochs to perform\n",
    "stats_interval = 1  # epoch interval between recording and printing stats\n",
    "hidden_dim = 50\n",
    "\n",
    "for index, weight_penalty in enumerate(weights_penalty_2):\n",
    "    rng.seed(seed)\n",
    "    train_data.reset()\n",
    "    valid_data.reset()\n",
    "\n",
    "    train_data.batch_size = batch_size \n",
    "    valid_data.batch_size = batch_size\n",
    "    \n",
    "    weights_init = GlorotUniformInit(rng=rng, gain=2.**0.5)\n",
    "    biases_init = ConstantInit(0.)\n",
    "    \n",
    "    \n",
    "    model = MultipleLayerModel([\n",
    "        AffineLayer(input_dim, hidden_dim, weights_init, biases_init, weight_penalty),\n",
    "        ReluLayer(),\n",
    "        AffineLayer(hidden_dim, output_dim, weights_init, biases_init, weight_penalty),\n",
    "    ])\n",
    "\n",
    "    error = SumOfSquaredDiffsError()\n",
    "    learning_rule = AdamLearningRule(learning_rate=learning_rate)\n",
    "    data_monitors={'acc': lambda y, t: (y.argmax(-1) == t.argmax(-1)).mean()}\n",
    "\n",
    "    optimiser = Optimiser(\n",
    "            model, error, learning_rule, train_data, valid_data, data_monitors)\n",
    "\n",
    "    stats, keys, run_time = optimiser.train(num_epochs=num_epochs, stats_interval=stats_interval)\n",
    "    \n",
    "    fig_1 = plt.figure(figsize=(8, 4))\n",
    "    ax_1 = fig_1.add_subplot(111)\n",
    "    plt.ylim([0.0305, 0.0973])\n",
    "    for k in ['error(train)', 'error(valid)']:\n",
    "        ax_1.plot(np.arange(1, stats.shape[0]) * stats_interval, \n",
    "                  stats[1:, keys[k]], label=k)\n",
    "    ax_1.legend(loc=0)\n",
    "    ax_1.set_xlabel('Epoch number')\n",
    "    plt.title(\"Training error and validation error\")\n",
    "    plt.savefig(\"{} TV error.pdf\".format(weight_penalty))\n",
    "    \n",
    "    print(\"Motion length: \", motion_data.shape[0])\n",
    "    result, evaluation = optimiser.eval_test_set(valid_data, 'validation')\n",
    "    prediction_300d_L2 = result[-1]\n",
    "    print('Validation Error with {}:    '.format(weight_penalty) + str(evaluation['errorvalidation']))\n",
    "    \n",
    "    prediction_L2.append(prediction_300d_L2)\n",
    "    \n",
    "    for i in range(6):\n",
    "        f, axarr = plt.subplots(2, sharex=True, figsize=(17,8))\n",
    "        axarr[0].plot(motion_data[:10000,i], color = 'cadetblue')\n",
    "        axarr[0].set_title('Actual motion data')\n",
    "        axarr[1].plot(prediction_300d_L2[:10000,i], color = 'cadetblue')\n",
    "        axarr[1].set_title('Raw prediction using L2 penalty')\n",
    "        plt.suptitle(\"Comparison of Actual Motion and Prediction on Dimension {}\".format(i+1), size=20)\n",
    "        plt.savefig(\"{0} ComparisonDim_{1}.pdf\".format(weight_penalty, i+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "motion_transpose = np.array(motion_data.transpose())\n",
    "prediction_transpose = np.array(prediction_L2[4].transpose())\n",
    "\n",
    "pearson_correlation_coefficient = np.corrcoef(motion_transpose, prediction_transpose)\n",
    "plot = sns.heatmap(pearson_correlation_coefficient, center=0, linewidths=.5)\n",
    "fig = plot.get_figure()\n",
    "fig.savefig(\"Heatmap Wiki\")\n",
    "for i in range(6):\n",
    "    print(\"CC of dimension {}\".format(i), \" is \", pearson_correlation_coefficient[i,i+6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test number of hidden dimensions (units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hidden_dims = [20, 50, 100, 150, 200]\n",
    "\n",
    "prediction_hidden_dim = []\n",
    "motion_data = np.load('data/Wiki/validation_extro.npz')['targets']\n",
    "\n",
    "\n",
    "batch_size = 100  # number of data points in a batch\n",
    "init_scale = 0.01  # scale for random parameter initialisation\n",
    "learning_rate = 0.001  # learning rate for gradient descent\n",
    "num_epochs = 30  # number of training epochs to perform\n",
    "stats_interval = 1  # epoch interval between recording and printing stats\n",
    "weight_penalty =  L2Penalty(1e-5)\n",
    "\n",
    "for hidden_dim in hidden_dims:\n",
    "    rng.seed(seed)\n",
    "    train_data.reset()\n",
    "    valid_data.reset()\n",
    "\n",
    "    train_data.batch_size = batch_size \n",
    "    valid_data.batch_size = batch_size\n",
    "    \n",
    "    weights_init = GlorotUniformInit(rng=rng, gain=2.**0.5)\n",
    "    biases_init = ConstantInit(0.)\n",
    "    \n",
    "    model = MultipleLayerModel([\n",
    "        AffineLayer(input_dim, hidden_dim, weights_init, biases_init, weight_penalty),\n",
    "        ReluLayer(),\n",
    "        AffineLayer(hidden_dim, output_dim, weights_init, biases_init, weight_penalty),\n",
    "    ])\n",
    "\n",
    "    error = SumOfSquaredDiffsError()\n",
    "    learning_rule = AdamLearningRule(learning_rate=learning_rate)\n",
    "    data_monitors={'acc': lambda y, t: (y.argmax(-1) == t.argmax(-1)).mean()}\n",
    "\n",
    "    optimiser = Optimiser(\n",
    "            model, error, learning_rule, train_data, valid_data, data_monitors)\n",
    "\n",
    "    stats, keys, run_time = optimiser.train(num_epochs=num_epochs, stats_interval=stats_interval)\n",
    "    \n",
    "    fig_1 = plt.figure(figsize=(8, 4))\n",
    "    ax_1 = fig_1.add_subplot(111)\n",
    "    plt.ylim([0.0305, 0.0973])\n",
    "    for k in ['error(train)', 'error(valid)']:\n",
    "        ax_1.plot(np.arange(1, stats.shape[0]) * stats_interval, \n",
    "                  stats[1:, keys[k]], label=k)\n",
    "    ax_1.legend(loc=0)\n",
    "    ax_1.set_xlabel('Epoch number')\n",
    "    plt.title(\"Training error and validation error\")\n",
    "    plt.savefig(\"{} units TVerror.pdf\".format(hidden_dim))\n",
    "        \n",
    "    result, evaluation = optimiser.eval_test_set(valid_data, 'validation')\n",
    "    prediction = result[-1]\n",
    "    print('Validation Error with {} hidden units:    '.format(hidden_dim) + str(evaluation['errorvalidation']))\n",
    "    \n",
    "    prediction_hidden_dim.append(prediction)\n",
    "    \n",
    "    for i in range(6):\n",
    "        f, axarr = plt.subplots(2, sharex=True, figsize=(17,8))\n",
    "        axarr[0].plot(motion_data[:10000,i], color = 'cadetblue')\n",
    "        axarr[0].set_title('Actual motion data')\n",
    "        axarr[1].plot(prediction[:10000,i], color = 'cadetblue')\n",
    "        axarr[1].set_title('Raw prediction with {} hidden units'.format(hidden_dim))\n",
    "        plt.suptitle(\"Comparison of Actual Motion and Prediction on Dimension {}\".format(i+1), size=20)\n",
    "        plt.savefig(\"{0} units ComparisonDim_{1}.pdf\".format(hidden_dim, i+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "motion_transpose = np.array(motion_data.transpose())\n",
    "prediction_transpose = np.array(prediction_hidden_dim[4].transpose())\n",
    "\n",
    "pearson_correlation_coefficient = np.corrcoef(motion_transpose, prediction_transpose)\n",
    "plot = sns.heatmap(pearson_correlation_coefficient, center=0, linewidths=.5)\n",
    "fig = plot.get_figure()\n",
    "fig.savefig(\"Heatmap Wiki\")\n",
    "for i in range(6):\n",
    "    print(\"CC of dimension {}\".format(i), \" is \", pearson_correlation_coefficient[i,i+6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 2 hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "motion_data = np.load('data/Wiki/validation_extro.npz')['targets']\n",
    "\n",
    "batch_size = 100  # number of data points in a batch\n",
    "init_scale = 0.01  # scale for random parameter initialisation\n",
    "learning_rate = 0.001  # learning rate for gradient descent\n",
    "num_epochs = 30  # number of training epochs to perform\n",
    "stats_interval = 1  # epoch interval between recording and printing stats\n",
    "weight_penalty =  L2Penalty(1e-5)\n",
    "hidden_dim = 100\n",
    "\n",
    "rng.seed(seed)\n",
    "train_data.reset()\n",
    "valid_data.reset()\n",
    "\n",
    "train_data.batch_size = batch_size \n",
    "valid_data.batch_size = batch_size\n",
    "\n",
    "weights_init = GlorotUniformInit(rng=rng, gain=2.**0.5)\n",
    "biases_init = ConstantInit(0.)\n",
    "\n",
    "model = MultipleLayerModel([\n",
    "    AffineLayer(input_dim, hidden_dim, weights_init, biases_init, weight_penalty),\n",
    "    ReluLayer(),\n",
    "    AffineLayer(hidden_dim, hidden_dim, weights_init, biases_init, weight_penalty),\n",
    "    ReluLayer(),    \n",
    "    AffineLayer(hidden_dim, output_dim, weights_init, biases_init, weight_penalty),\n",
    "])\n",
    "\n",
    "error = SumOfSquaredDiffsError()\n",
    "learning_rule = AdamLearningRule(learning_rate=learning_rate)\n",
    "data_monitors={'acc': lambda y, t: (y.argmax(-1) == t.argmax(-1)).mean()}\n",
    "\n",
    "optimiser = Optimiser(\n",
    "        model, error, learning_rule, train_data, valid_data, data_monitors)\n",
    "\n",
    "stats, keys, run_time = optimiser.train(num_epochs=num_epochs, stats_interval=stats_interval)\n",
    "\n",
    "fig_1 = plt.figure(figsize=(8, 4))\n",
    "ax_1 = fig_1.add_subplot(111)\n",
    "# plt.ylim([0.0305, 0.0973])\n",
    "for k in ['error(train)', 'error(valid)']:\n",
    "    ax_1.plot(np.arange(1, stats.shape[0]) * stats_interval, \n",
    "              stats[1:, keys[k]], label=k)\n",
    "ax_1.legend(loc=0)\n",
    "ax_1.set_xlabel('Epoch number')\n",
    "plt.title(\"Training error and validation error\")\n",
    "plt.savefig(\"100 units 2 layers TVerror.pdf\")\n",
    "\n",
    "result, evaluation = optimiser.eval_test_set(valid_data, 'validation')\n",
    "prediction = result[-1]\n",
    "print('Validation Error with 2 hidden layers:    ' + str(evaluation['errorvalidation']))\n",
    "\n",
    "for i in range(6):\n",
    "    f, axarr = plt.subplots(2, sharex=True, figsize=(17,8))\n",
    "    axarr[0].plot(motion_data[:10000,i], color = 'cadetblue')\n",
    "    axarr[0].set_title('Actual motion data')\n",
    "    axarr[1].plot(prediction[:10000,i], color = 'cadetblue')\n",
    "    axarr[1].set_title('Raw prediction with {} hidden units'.format(hidden_dim))\n",
    "    plt.suptitle(\"Comparison of Actual Motion and Prediction on Dimension {}\".format(i+1), size=20)\n",
    "    plt.savefig(\"{0} units ComparisonDim_{1}.pdf\".format(hidden_dim, i+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "motion_transpose = np.array(motion_data.transpose())\n",
    "prediction_transpose = np.array(prediction.transpose())\n",
    "\n",
    "pearson_correlation_coefficient = np.corrcoef(motion_transpose, prediction_transpose)\n",
    "plot = sns.heatmap(pearson_correlation_coefficient, center=0, linewidths=.5)\n",
    "fig = plot.get_figure()\n",
    "fig.savefig(\"Heatmap 2 layers\")\n",
    "for i in range(6):\n",
    "    print(\"CC of dimension {}\".format(i), \" is \", pearson_correlation_coefficient[i,i+6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "incl_probs = [0.2, 0.5, 0.8]\n",
    "motion_data = np.load('data/Wiki/validation_extro.npz')['targets']\n",
    "predictions = []\n",
    "\n",
    "batch_size = 100  \n",
    "init_scale = 0.01  \n",
    "learning_rate = 0.001  \n",
    "num_epochs = 50 \n",
    "stats_interval = 1 \n",
    "weight_penalty = L2Penalty(1e-5)\n",
    "hidden_dim = 150\n",
    "\n",
    "for incl_prob in incl_probs:\n",
    "    rng.seed(seed)\n",
    "    train_data.reset()\n",
    "    valid_data.reset()\n",
    "\n",
    "    train_data.batch_size = batch_size \n",
    "    valid_data.batch_size = batch_size\n",
    "\n",
    "    weights_init = GlorotUniformInit(rng=rng, gain=2.**0.5)\n",
    "    biases_init = ConstantInit(0.)\n",
    "\n",
    "    model = MultipleLayerModel([\n",
    "        DropoutLayer(rng, incl_prob),\n",
    "        AffineLayer(input_dim, hidden_dim, weights_init, biases_init, weight_penalty),\n",
    "        ReluLayer(),\n",
    "        DropoutLayer(rng, incl_prob),\n",
    "        AffineLayer(hidden_dim, output_dim, weights_init, biases_init, weight_penalty),\n",
    "    ])\n",
    "\n",
    "    error = SumOfSquaredDiffsError()\n",
    "    learning_rule = AdamLearningRule(learning_rate=learning_rate)\n",
    "    data_monitors={'acc': lambda y, t: (y.argmax(-1) == t.argmax(-1)).mean()}\n",
    "\n",
    "    optimiser = Optimiser(\n",
    "            model, error, learning_rule, train_data, valid_data, data_monitors)\n",
    "\n",
    "    stats, keys, run_time = optimiser.train(num_epochs=num_epochs, stats_interval=stats_interval)\n",
    "\n",
    "    fig_1 = plt.figure(figsize=(8, 4))\n",
    "    ax_1 = fig_1.add_subplot(111)\n",
    "    # plt.ylim([0.0305, 0.0973])\n",
    "    for k in ['error(train)', 'error(valid)']:\n",
    "        ax_1.plot(np.arange(1, stats.shape[0]) * stats_interval, \n",
    "                  stats[1:, keys[k]], label=k)\n",
    "    ax_1.legend(loc=0)\n",
    "    ax_1.set_xlabel('Epoch number')\n",
    "    plt.title(\"Training error and validation error\")\n",
    "    plt.savefig(\"Dropout {} TVerror.pdf\".format(incl_prob))\n",
    "\n",
    "    result, evaluation = optimiser.eval_test_set(valid_data, 'validation')\n",
    "    prediction = result[-1]\n",
    "    predictions.append(prediction)\n",
    "    print('Validation Error with dropout:    ' + str(evaluation['errorvalidation']))\n",
    "\n",
    "    for i in range(6):\n",
    "        f, axarr = plt.subplots(2, sharex=True, figsize=(17,8))\n",
    "        axarr[0].plot(motion_data[:10000,i], color = 'cadetblue')\n",
    "        axarr[0].set_title('Actual motion data')\n",
    "        axarr[1].plot(prediction[:10000,i], color = 'cadetblue')\n",
    "        axarr[1].set_title('Raw prediction with {} hidden units'.format(hidden_dim))\n",
    "        plt.suptitle(\"Comparison of Actual Motion and Prediction on Dimension {}\".format(i+1), size=20)\n",
    "        plt.savefig(\"Drop {0} ComparisonDim_{1}.pdf\".format(incl_prob, i+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "motion_transpose = np.array(motion_data.transpose())\n",
    "prediction_transpose = np.array(prediction.transpose())\n",
    "\n",
    "pearson_correlation_coefficient = np.corrcoef(motion_transpose, prediction_transpose)\n",
    "plot = sns.heatmap(pearson_correlation_coefficient, center=0, linewidths=.5)\n",
    "fig = plot.get_figure()\n",
    "fig.savefig(\"Dropout Heatmap\")\n",
    "for i in range(6):\n",
    "    print(\"CC of dimension {}\".format(i), \" is \", pearson_correlation_coefficient[i,i+6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Set training run hyperparameters\n",
    "batch_size = 100  # number of data points in a batch\n",
    "init_scale = 0.01  # scale for random parameter initialisation\n",
    "learning_rate = 0.001  # learning rate for gradient descent\n",
    "num_epochs = 30  # number of training epochs to perform\n",
    "stats_interval = 1  # epoch interval between recording and printing stats\n",
    "hidden_dim = 50\n",
    "incl_prob = 0.5\n",
    "\n",
    "weights_penalty_1 = L1Penalty(1e-8)\n",
    "weights_penalty_2 = L2Penalty(1e-5)\n",
    "\n",
    "# Reset random number generator and data provider states on each run\n",
    "# to ensure reproducibility of results\n",
    "rng.seed(seed)\n",
    "train_data.reset()\n",
    "valid_data.reset()\n",
    "\n",
    "# Alter data-provider batch size\n",
    "train_data.batch_size = batch_size \n",
    "valid_data.batch_size = batch_size\n",
    "\n",
    "# Create a parameter initialiser which will sample random uniform values\n",
    "# from [-init_scale, init_scale]\n",
    "# param_init = UniformInit(-init_scale, init_scale, rng=rng)\n",
    "\n",
    "weights_init = GlorotUniformInit(rng=rng, gain=2.**0.5)\n",
    "biases_init = ConstantInit(0.)\n",
    "\n",
    "# Create affine + softmax model\n",
    "model = MultipleLayerModel([\n",
    "    #DropoutLayer(rng, incl_prob),\n",
    "    AffineLayer(input_dim, hidden_dim, weights_init, biases_init, weights_penalty_2),\n",
    "#     #BatchNormalizationLayer(input_dim = hidden_dim),\n",
    "#     ReluLayer(),\n",
    "#     #DropoutLayer(rng, incl_prob),\n",
    "#     AffineLayer(hidden_dim, hidden_dim, weights_init, biases_init),\n",
    "#     #BatchNormalizationLayer(input_dim = hidden_dim),\n",
    "    ReluLayer(),\n",
    "    #DropoutLayer(rng, incl_prob),\n",
    "    AffineLayer(hidden_dim, output_dim, weights_init, biases_init, weights_penalty_2),\n",
    "])\n",
    "\n",
    "# Initialise a cross entropy error object\n",
    "error = SumOfSquaredDiffsError()\n",
    "# error = L1Error()\n",
    "\n",
    "# Use Adam learning rule learning rule\n",
    "learning_rule = AdamLearningRule(learning_rate=learning_rate)\n",
    "\n",
    "\n",
    "data_monitors={'acc': lambda y, t: (y.argmax(-1) == t.argmax(-1)).mean()}\n",
    "\n",
    "optimiser = Optimiser(\n",
    "        model, error, learning_rule, train_data, valid_data, data_monitors)\n",
    "\n",
    "stats, keys, run_time = optimiser.train(num_epochs=num_epochs, stats_interval=stats_interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_1 = plt.figure(figsize=(8, 4))\n",
    "ax_1 = fig_1.add_subplot(111)\n",
    "plt.ylim([0.0305, 0.0973])\n",
    "for k in ['error(train)', 'error(valid)']:\n",
    "    ax_1.plot(np.arange(1, stats.shape[0]) * stats_interval, \n",
    "              stats[1:, keys[k]], label=k)\n",
    "ax_1.legend(loc=0)\n",
    "ax_1.set_xlabel('Epoch number')\n",
    "plt.title(\"TVerror\")\n",
    "plt.savefig(\"{0}.pdf\".format(weights_penalty_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize validation set prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "motion_data = np.load('data/Wiki/validation_extro.npz')['targets']\n",
    "print(\"Motion length: \", motion_data.shape[0])\n",
    "result, evaluation = optimiser.eval_test_set(valid_data, 'validation')\n",
    "prediction_300d_L2 = result[-1]\n",
    "print('Error:    ' + str(evaluation['errorvalidation']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(6):\n",
    "    f, axarr = plt.subplots(2, sharex=True, figsize=(17,8))\n",
    "    axarr[0].plot(motion_data[:10000,i], color = 'cadetblue')\n",
    "    axarr[0].set_title('Actual motion data')\n",
    "#     axarr[1].plot(prediction_300d_L1[:10000,i], color = 'cadetblue')\n",
    "#     axarr[1].set_title('Raw prediction without penalties')\n",
    "    axarr[1].plot(prediction_300d_L2[:10000,i], color = 'cadetblue')\n",
    "    axarr[1].set_title('Raw prediction using L1 penalty')\n",
    "    plt.suptitle(\"Comparison of Actual Motion and Prediction on Dimension {}\".format(i+1), size=20)\n",
    "    plt.savefig(\"ComparisonDim{}.pdf\".format(i+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Correlation Coefficient of validation set prediction and actual motion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "motion_transpose = np.array(motion_data.transpose())\n",
    "#prediction_transpose = np.array(prediction_Twitter.transpose())\n",
    "prediction_transpose = np.array(prediction_300d_L2.transpose())\n",
    "\n",
    "pearson_correlation_coefficient = np.corrcoef(motion_transpose, prediction_transpose)\n",
    "plot = sns.heatmap(pearson_correlation_coefficient, center=0, linewidths=.5)\n",
    "fig = plot.get_figure()\n",
    "fig.savefig(\"Heatmap Wiki\")\n",
    "for i in range(6):\n",
    "    print(\"CC of dimension {}\".format(i), \" is \", pearson_correlation_coefficient[i,i+6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a function that smooths the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def smooth_prediction(raw_prediction):\n",
    "    output_shape = raw_prediction.shape\n",
    "    # make a matrix that adds 20 lines paddings at the beginning & end of raw prediction\n",
    "    calculation_frame = np.zeros((output_shape[0]+40, output_shape[1]))\n",
    "    output = np.zeros(raw_prediction.shape)\n",
    "    calculation_frame[20:-20, :] = raw_prediction\n",
    "\n",
    "    for i in range(output_shape[0]):\n",
    "        output[i,:] = calculation_frame[i+20,:] + \\\n",
    "        0.9 * (calculation_frame[i+21,:] + calculation_frame[i+19,:]) + \\\n",
    "        0.9 * (calculation_frame[i+22,:] + calculation_frame[i+18,:]) + \\\n",
    "        0.8 * (calculation_frame[i+23,:] + calculation_frame[i+17,:]) + \\\n",
    "        0.8 * (calculation_frame[i+24,:] + calculation_frame[i+16,:]) + \\\n",
    "        0.7 * (calculation_frame[i+25,:] + calculation_frame[i+15,:]) + \\\n",
    "        0.7 * (calculation_frame[i+26,:] + calculation_frame[i+14,:]) + \\\n",
    "        0.6 * (calculation_frame[i+27,:] + calculation_frame[i+13,:]) + \\\n",
    "        0.6 * (calculation_frame[i+28,:] + calculation_frame[i+12,:]) + \\\n",
    "        0.5 * (calculation_frame[i+29,:] + calculation_frame[i+11,:]) + \\\n",
    "        0.5 * (calculation_frame[i+30,:] + calculation_frame[i+10,:]) + \\\n",
    "        0.4 * (calculation_frame[i+31,:] + calculation_frame[i+9,:]) + \\\n",
    "        0.4 * (calculation_frame[i+32,:] + calculation_frame[i+8,:]) + \\\n",
    "        0.3 * (calculation_frame[i+33,:] + calculation_frame[i+7,:]) + \\\n",
    "        0.3 * (calculation_frame[i+34,:] + calculation_frame[i+6,:]) + \\\n",
    "        0.2 * (calculation_frame[i+35,:] + calculation_frame[i+5,:]) + \\\n",
    "        0.2 * (calculation_frame[i+36,:] + calculation_frame[i+4,:]) + \\\n",
    "        0.1 * (calculation_frame[i+37,:] + calculation_frame[i+3,:]) + \\\n",
    "        0.1 * (calculation_frame[i+38,:] + calculation_frame[i+2,:]) + \\\n",
    "        0.1 * (calculation_frame[i+39,:] + calculation_frame[i+1,:])\n",
    "#     for i in range(output_shape[0]):\n",
    "#         output[i,:] = 1.5 * calculation_frame[i+20,:] + \\\n",
    "#         0.5 * (calculation_frame[i+21,:] + calculation_frame[i+19,:]) + \\\n",
    "#         0.5 * (calculation_frame[i+22,:] + calculation_frame[i+18,:]) + \\\n",
    "#         0.3 * (calculation_frame[i+23,:] + calculation_frame[i+17,:]) + \\\n",
    "#         0.3 * (calculation_frame[i+24,:] + calculation_frame[i+16,:]) + \\\n",
    "#         0.1 * (calculation_frame[i+25,:] + calculation_frame[i+15,:])\n",
    "    output /= 2\n",
    "\n",
    "    # Adjust motion to origin\n",
    "#     motion_mean = np.mean(output,axis=0)\n",
    "#     output = output - motion_mean\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dct_smoothing(raw_prediction, frequency_factor, window_size):\n",
    "    trans = raw_prediction.transpose()\n",
    "    output = np.zeros(trans.shape)\n",
    "    \n",
    "    window_number = (output.shape[1] + window_size - 1) // window_size\n",
    "    temp_output = np.zeros((trans.shape[0], window_size*window_number))\n",
    "    \n",
    "    # Do dct smoothing on every window individually\n",
    "    for i in range(trans.shape[0]):\n",
    "        motion = np.zeros(window_size * window_number)\n",
    "        motion[:trans.shape[1]] = trans[i,:]\n",
    "        for j in range(window_number):\n",
    "            temp = np.zeros(window_size)\n",
    "            temp_freq = np.zeros(window_size)\n",
    "            temp_freq[:frequency_factor] = dct(motion[j*window_size:(j+1)*window_size], norm=\"ortho\")[:frequency_factor]\n",
    "            temp_output[i, j*window_size:(j+1)*window_size] = dct(temp_freq, 3, norm=\"ortho\")\n",
    "            #ith_dim_freq = np.zeros(trans.shape[1])\n",
    "            #ith_dim_freq[:frequency_factor] = dct(trans[i], norm=\"ortho\")[:frequency_factor]\n",
    "            #output[i] = dct(ith_dim_freq, 3, norm=\"ortho\")\n",
    "    \n",
    "    output[:, :raw_prediction.shape[0]] = temp_output[:, :raw_prediction.shape[0]]  \n",
    "    output = output.transpose()\n",
    "    \n",
    "    smooth_boundary = output.copy()\n",
    "    \n",
    "    # Smooth the boundaries\n",
    "#     for i in range(1, window_number):\n",
    "#         mid_left = i * window_size-1\n",
    "#         mid_right = mid_left + 1\n",
    "#         start = mid_left - 29\n",
    "#         end = mid_right + 29\n",
    "        \n",
    "#         mid_mean = (output[mid_left] + output[mid_right]) / 2.0\n",
    "        \n",
    "#         for j in range(31):\n",
    "#             smooth_boundary[start + j,:] = (1 - j / 30.0) * output[j] + j / 30.0 * mid_mean\n",
    "#             smooth_boundary[end - j,:] = (1 - j / 30.0) * output[j] + j / 30.0 * mid_mean\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Produce output of test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Loop over 1-6 test cases\n",
    "\n",
    "for i in range(1,7):\n",
    "    test_data = HMSDataProvider('test{0}'.format(i), rng=rng)\n",
    "    result, evaluation = optimiser.eval_test_set(test_data, 'test')\n",
    "    print('Error:    ' + str(evaluation['errortest']))\n",
    "    \n",
    "    time_intervals = np.loadtxt(\"ExtrovertRawData/Words/{0}\".format(i), usecols=range(4, 6), dtype=\"int\")\n",
    "    prediction = np.zeros((time_intervals[-1,1] + 300, 6))\n",
    "    counter = 0\n",
    "    for line in time_intervals:\n",
    "        for j in range(line[0], line[1]):\n",
    "            prediction[j] = result[-1][counter]\n",
    "            counter += 1\n",
    "    prediction_smooth = smooth_prediction(prediction)\n",
    "    prediction_dct_smooth = dct_smoothing(prediction, frequency_factor=5, window_size=100)\n",
    "    prediction_dct_smooth *= 5\n",
    "\n",
    "    np.savetxt('Predictions/extro_{0}.txt'.format(i), prediction, fmt=\"%.7f\")\n",
    "    np.savetxt('Predictions/extro_smooth_{0}.txt'.format(i), prediction_smooth, fmt=\"%.7f\")\n",
    "    np.savetxt('Predictions/extro_dct_{0}.txt'.format(i), prediction_dct_smooth, fmt=\"%.7f\")\n",
    "    \n",
    "    motion_data = np.loadtxt(\"ExtrovertRawData/Motion/{0}.rov\".format(i), skiprows=17, usecols=range(0, 6))\n",
    "    print(\"Motion length: \", motion_data.shape[0])\n",
    "\n",
    "    # Two subplots, the axes array is 1-d\n",
    "    f, axarr = plt.subplots(4, sharex=True, figsize=(17,8))\n",
    "    axarr[0].plot(motion_data[:5000,i-1], color = 'cadetblue')\n",
    "    axarr[0].set_title('Actual motion data')\n",
    "    axarr[1].plot(prediction[:5000,i-1], color = 'cadetblue')\n",
    "    axarr[1].set_title('Raw prediction')\n",
    "    axarr[2].plot(prediction_smooth[:5000,i-1], color = 'cadetblue')\n",
    "    axarr[2].set_title('Smoothed prediction')\n",
    "    axarr[3].plot(prediction_dct_smooth[:5000,i-1], color = 'cadetblue')\n",
    "    axarr[3].set_title('DCT Smoothed prediction')\n",
    "    plt.suptitle('Test case {0}, comparison on dimention {1}'.format(i, i), size = 20)\n",
    "    #plt.savefig('Predictions/Test case {0}, comparison on dimention {1}.pdf'.format(i, i))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
