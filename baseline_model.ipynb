{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "from hms.layers import AffineLayer, SoftmaxLayer, SigmoidLayer, ReluLayer, LeakyReluLayer, DropoutLayer, BatchNormalizationLayer\n",
    "from hms.errors import CrossEntropyError, CrossEntropySoftmaxError, SumOfSquaredDiffsError, L1Error, L2Error\n",
    "from hms.models import SingleLayerModel, MultipleLayerModel\n",
    "from hms.initialisers import UniformInit, GlorotUniformInit, ConstantInit\n",
    "from hms.learning_rules import GradientDescentLearningRule, AdamLearningRule\n",
    "from hms.data_providers import HMSDataProvider, HMS300dDataProvider\n",
    "from hms.optimisers import Optimiser\n",
    "from hms.penalties import L1Penalty, L2Penalty\n",
    "import seaborn as sns;\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Seed a random number generator\n",
    "seed = 6102016 \n",
    "rng = np.random.RandomState(seed)\n",
    "\n",
    "# Set up a logger object to print info about the training run to stdout\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.handlers = [logging.StreamHandler()]\n",
    "\n",
    "# Create data provider objects for the MNIST data set\n",
    "train_data = HMS300dDataProvider('train', 'intro', 'Wiki', rng=rng)\n",
    "valid_data = HMS300dDataProvider('validation', 'intro', 'Wiki', rng=rng)\n",
    "input_dim, output_dim = 300, 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Penalty coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weights_penalty_2 = [L2Penalty(1e-1), L2Penalty(1e-2), L2Penalty(1e-4), L2Penalty(1e-6), L2Penalty(1e-8)]\n",
    "prediction_L2 = []\n",
    "motion_data = np.load('data/Wiki/validation_intro.npz')['targets']\n",
    "\n",
    "batch_size = 100  # number of data points in a batch\n",
    "init_scale = 0.01  # scale for random parameter initialisation\n",
    "learning_rate = 0.001  # learning rate for gradient descent\n",
    "num_epochs = 30  # number of training epochs to perform\n",
    "stats_interval = 1  # epoch interval between recording and printing stats\n",
    "hidden_dim = 50\n",
    "\n",
    "for index, weight_penalty in enumerate(weights_penalty_2):\n",
    "    rng.seed(seed)\n",
    "    train_data.reset()\n",
    "    valid_data.reset()\n",
    "\n",
    "    train_data.batch_size = batch_size \n",
    "    valid_data.batch_size = batch_size\n",
    "    \n",
    "    weights_init = GlorotUniformInit(rng=rng, gain=2.**0.5)\n",
    "    biases_init = ConstantInit(0.)\n",
    "    \n",
    "    \n",
    "    model = MultipleLayerModel([\n",
    "        AffineLayer(input_dim, hidden_dim, weights_init, biases_init, weight_penalty),\n",
    "        ReluLayer(),\n",
    "        AffineLayer(hidden_dim, output_dim, weights_init, biases_init, weight_penalty),\n",
    "    ])\n",
    "\n",
    "    error = SumOfSquaredDiffsError()\n",
    "    learning_rule = AdamLearningRule(learning_rate=learning_rate)\n",
    "    data_monitors={'acc': lambda y, t: (y.argmax(-1) == t.argmax(-1)).mean()}\n",
    "\n",
    "    optimiser = Optimiser(\n",
    "            model, error, learning_rule, train_data, valid_data, data_monitors)\n",
    "\n",
    "    stats, keys, run_time = optimiser.train(num_epochs=num_epochs, stats_interval=stats_interval)\n",
    "    \n",
    "    fig_1 = plt.figure(figsize=(8, 4))\n",
    "    ax_1 = fig_1.add_subplot(111)\n",
    "    plt.ylim([0.0305, 0.0973])\n",
    "    for k in ['error(train)', 'error(valid)']:\n",
    "        ax_1.plot(np.arange(1, stats.shape[0]) * stats_interval, \n",
    "                  stats[1:, keys[k]], label=k)\n",
    "    ax_1.legend(loc=0)\n",
    "    ax_1.set_xlabel('Epoch number')\n",
    "    plt.title(\"Training error and validation error\")\n",
    "    plt.savefig(\"{} TV error.pdf\".format(weight_penalty))\n",
    "    \n",
    "    print(\"Motion length: \", motion_data.shape[0])\n",
    "    result, evaluation = optimiser.eval_test_set(valid_data, 'validation')\n",
    "    prediction_300d_L2 = result[-1]\n",
    "    print('Validation Error with {}:    '.format(weight_penalty) + str(evaluation['errorvalidation']))\n",
    "    \n",
    "    prediction_L2.append(prediction_300d_L2)\n",
    "    \n",
    "    for i in range(6):\n",
    "        f, axarr = plt.subplots(2, sharex=True, figsize=(17,8))\n",
    "        axarr[0].plot(motion_data[:10000,i], color = 'cadetblue')\n",
    "        axarr[0].set_title('Actual motion data')\n",
    "        axarr[1].plot(prediction_300d_L2[:10000,i], color = 'cadetblue')\n",
    "        axarr[1].set_title('Raw prediction using L2 penalty')\n",
    "        plt.suptitle(\"Comparison of Actual Motion and Prediction on Dimension {}\".format(i+1), size=20)\n",
    "        plt.savefig(\"{0} ComparisonDim_{1}.pdf\".format(weight_penalty, i+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "motion_transpose = np.array(motion_data.transpose())\n",
    "prediction_transpose = np.array(prediction_L2[4].transpose())\n",
    "\n",
    "pearson_correlation_coefficient = np.corrcoef(motion_transpose, prediction_transpose)\n",
    "plot = sns.heatmap(pearson_correlation_coefficient, center=0, linewidths=.5)\n",
    "fig = plot.get_figure()\n",
    "fig.savefig(\"Heatmap Wiki\")\n",
    "for i in range(6):\n",
    "    print(\"CC of dimension {}\".format(i), \" is \", pearson_correlation_coefficient[i,i+6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test number of hidden dimensions (units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hidden_dims = [20, 50, 100, 150, 200]\n",
    "\n",
    "prediction_hidden_dim = []\n",
    "motion_data = np.load('data/Wiki/validation_extro.npz')['targets']\n",
    "\n",
    "\n",
    "batch_size = 100  # number of data points in a batch\n",
    "init_scale = 0.01  # scale for random parameter initialisation\n",
    "learning_rate = 0.001  # learning rate for gradient descent\n",
    "num_epochs = 30  # number of training epochs to perform\n",
    "stats_interval = 1  # epoch interval between recording and printing stats\n",
    "weight_penalty =  L2Penalty(1e-5)\n",
    "\n",
    "for hidden_dim in hidden_dims:\n",
    "    rng.seed(seed)\n",
    "    train_data.reset()\n",
    "    valid_data.reset()\n",
    "\n",
    "    train_data.batch_size = batch_size \n",
    "    valid_data.batch_size = batch_size\n",
    "    \n",
    "    weights_init = GlorotUniformInit(rng=rng, gain=2.**0.5)\n",
    "    biases_init = ConstantInit(0.)\n",
    "    \n",
    "    model = MultipleLayerModel([\n",
    "        AffineLayer(input_dim, hidden_dim, weights_init, biases_init, weight_penalty),\n",
    "        ReluLayer(),\n",
    "        AffineLayer(hidden_dim, output_dim, weights_init, biases_init, weight_penalty),\n",
    "    ])\n",
    "\n",
    "    error = SumOfSquaredDiffsError()\n",
    "    learning_rule = AdamLearningRule(learning_rate=learning_rate)\n",
    "    data_monitors={'acc': lambda y, t: (y.argmax(-1) == t.argmax(-1)).mean()}\n",
    "\n",
    "    optimiser = Optimiser(\n",
    "            model, error, learning_rule, train_data, valid_data, data_monitors)\n",
    "\n",
    "    stats, keys, run_time = optimiser.train(num_epochs=num_epochs, stats_interval=stats_interval)\n",
    "    \n",
    "    fig_1 = plt.figure(figsize=(8, 4))\n",
    "    ax_1 = fig_1.add_subplot(111)\n",
    "    plt.ylim([0.0305, 0.0973])\n",
    "    for k in ['error(train)', 'error(valid)']:\n",
    "        ax_1.plot(np.arange(1, stats.shape[0]) * stats_interval, \n",
    "                  stats[1:, keys[k]], label=k)\n",
    "    ax_1.legend(loc=0)\n",
    "    ax_1.set_xlabel('Epoch number')\n",
    "    plt.title(\"Training error and validation error\")\n",
    "    plt.savefig(\"{} units TVerror.pdf\".format(hidden_dim))\n",
    "        \n",
    "    result, evaluation = optimiser.eval_test_set(valid_data, 'validation')\n",
    "    prediction = result[-1]\n",
    "    print('Validation Error with {} hidden units:    '.format(hidden_dim) + str(evaluation['errorvalidation']))\n",
    "    \n",
    "    prediction_hidden_dim.append(prediction)\n",
    "    \n",
    "    for i in range(6):\n",
    "        f, axarr = plt.subplots(2, sharex=True, figsize=(17,8))\n",
    "        axarr[0].plot(motion_data[:10000,i], color = 'cadetblue')\n",
    "        axarr[0].set_title('Actual motion data')\n",
    "        axarr[1].plot(prediction[:10000,i], color = 'cadetblue')\n",
    "        axarr[1].set_title('Raw prediction with {} hidden units'.format(hidden_dim))\n",
    "        plt.suptitle(\"Comparison of Actual Motion and Prediction on Dimension {}\".format(i+1), size=20)\n",
    "        plt.savefig(\"{0} units ComparisonDim_{1}.pdf\".format(hidden_dim, i+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "motion_transpose = np.array(motion_data.transpose())\n",
    "prediction_transpose = np.array(prediction_hidden_dim[4].transpose())\n",
    "\n",
    "pearson_correlation_coefficient = np.corrcoef(motion_transpose, prediction_transpose)\n",
    "plot = sns.heatmap(pearson_correlation_coefficient, center=0, linewidths=.5)\n",
    "fig = plot.get_figure()\n",
    "fig.savefig(\"Heatmap Wiki\")\n",
    "for i in range(6):\n",
    "    print(\"CC of dimension {}\".format(i), \" is \", pearson_correlation_coefficient[i,i+6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 2 hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "motion_data = np.load('data/Wiki/validation_extro.npz')['targets']\n",
    "\n",
    "batch_size = 100  # number of data points in a batch\n",
    "init_scale = 0.01  # scale for random parameter initialisation\n",
    "learning_rate = 0.001  # learning rate for gradient descent\n",
    "num_epochs = 30  # number of training epochs to perform\n",
    "stats_interval = 1  # epoch interval between recording and printing stats\n",
    "weight_penalty =  L2Penalty(1e-5)\n",
    "hidden_dim = 100\n",
    "\n",
    "rng.seed(seed)\n",
    "train_data.reset()\n",
    "valid_data.reset()\n",
    "\n",
    "train_data.batch_size = batch_size \n",
    "valid_data.batch_size = batch_size\n",
    "\n",
    "weights_init = GlorotUniformInit(rng=rng, gain=2.**0.5)\n",
    "biases_init = ConstantInit(0.)\n",
    "\n",
    "model = MultipleLayerModel([\n",
    "    AffineLayer(input_dim, hidden_dim, weights_init, biases_init, weight_penalty),\n",
    "    ReluLayer(),\n",
    "    AffineLayer(hidden_dim, hidden_dim, weights_init, biases_init, weight_penalty),\n",
    "    ReluLayer(),    \n",
    "    AffineLayer(hidden_dim, output_dim, weights_init, biases_init, weight_penalty),\n",
    "])\n",
    "\n",
    "error = SumOfSquaredDiffsError()\n",
    "learning_rule = AdamLearningRule(learning_rate=learning_rate)\n",
    "data_monitors={'acc': lambda y, t: (y.argmax(-1) == t.argmax(-1)).mean()}\n",
    "\n",
    "optimiser = Optimiser(\n",
    "        model, error, learning_rule, train_data, valid_data, data_monitors)\n",
    "\n",
    "stats, keys, run_time = optimiser.train(num_epochs=num_epochs, stats_interval=stats_interval)\n",
    "\n",
    "fig_1 = plt.figure(figsize=(8, 4))\n",
    "ax_1 = fig_1.add_subplot(111)\n",
    "# plt.ylim([0.0305, 0.0973])\n",
    "for k in ['error(train)', 'error(valid)']:\n",
    "    ax_1.plot(np.arange(1, stats.shape[0]) * stats_interval, \n",
    "              stats[1:, keys[k]], label=k)\n",
    "ax_1.legend(loc=0)\n",
    "ax_1.set_xlabel('Epoch number')\n",
    "plt.title(\"Training error and validation error\")\n",
    "plt.savefig(\"100 units 2 layers TVerror.pdf\")\n",
    "\n",
    "result, evaluation = optimiser.eval_test_set(valid_data, 'validation')\n",
    "prediction = result[-1]\n",
    "print('Validation Error with 2 hidden layers:    ' + str(evaluation['errorvalidation']))\n",
    "\n",
    "for i in range(6):\n",
    "    f, axarr = plt.subplots(2, sharex=True, figsize=(17,8))\n",
    "    axarr[0].plot(motion_data[:10000,i], color = 'cadetblue')\n",
    "    axarr[0].set_title('Actual motion data')\n",
    "    axarr[1].plot(prediction[:10000,i], color = 'cadetblue')\n",
    "    axarr[1].set_title('Raw prediction with {} hidden units'.format(hidden_dim))\n",
    "    plt.suptitle(\"Comparison of Actual Motion and Prediction on Dimension {}\".format(i+1), size=20)\n",
    "    plt.savefig(\"{0} units ComparisonDim_{1}.pdf\".format(hidden_dim, i+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "motion_transpose = np.array(motion_data.transpose())\n",
    "prediction_transpose = np.array(prediction.transpose())\n",
    "\n",
    "pearson_correlation_coefficient = np.corrcoef(motion_transpose, prediction_transpose)\n",
    "plot = sns.heatmap(pearson_correlation_coefficient, center=0, linewidths=.5)\n",
    "fig = plot.get_figure()\n",
    "fig.savefig(\"Heatmap 2 layers\")\n",
    "for i in range(6):\n",
    "    print(\"CC of dimension {}\".format(i), \" is \", pearson_correlation_coefficient[i,i+6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "incl_probs = [0.2, 0.5, 0.8]\n",
    "motion_data = np.load('data/Wiki/validation_extro.npz')['targets']\n",
    "predictions = []\n",
    "\n",
    "batch_size = 100  \n",
    "init_scale = 0.01  \n",
    "learning_rate = 0.001  \n",
    "num_epochs = 50 \n",
    "stats_interval = 1 \n",
    "weight_penalty = L2Penalty(1e-5)\n",
    "hidden_dim = 150\n",
    "\n",
    "for incl_prob in incl_probs:\n",
    "    rng.seed(seed)\n",
    "    train_data.reset()\n",
    "    valid_data.reset()\n",
    "\n",
    "    train_data.batch_size = batch_size \n",
    "    valid_data.batch_size = batch_size\n",
    "\n",
    "    weights_init = GlorotUniformInit(rng=rng, gain=2.**0.5)\n",
    "    biases_init = ConstantInit(0.)\n",
    "\n",
    "    model = MultipleLayerModel([\n",
    "        DropoutLayer(rng, incl_prob),\n",
    "        AffineLayer(input_dim, hidden_dim, weights_init, biases_init, weight_penalty),\n",
    "        ReluLayer(),\n",
    "        DropoutLayer(rng, incl_prob),\n",
    "        AffineLayer(hidden_dim, output_dim, weights_init, biases_init, weight_penalty),\n",
    "    ])\n",
    "\n",
    "    error = SumOfSquaredDiffsError()\n",
    "    learning_rule = AdamLearningRule(learning_rate=learning_rate)\n",
    "    data_monitors={'acc': lambda y, t: (y.argmax(-1) == t.argmax(-1)).mean()}\n",
    "\n",
    "    optimiser = Optimiser(\n",
    "            model, error, learning_rule, train_data, valid_data, data_monitors)\n",
    "\n",
    "    stats, keys, run_time = optimiser.train(num_epochs=num_epochs, stats_interval=stats_interval)\n",
    "\n",
    "    fig_1 = plt.figure(figsize=(8, 4))\n",
    "    ax_1 = fig_1.add_subplot(111)\n",
    "    # plt.ylim([0.0305, 0.0973])\n",
    "    for k in ['error(train)', 'error(valid)']:\n",
    "        ax_1.plot(np.arange(1, stats.shape[0]) * stats_interval, \n",
    "                  stats[1:, keys[k]], label=k)\n",
    "    ax_1.legend(loc=0)\n",
    "    ax_1.set_xlabel('Epoch number')\n",
    "    plt.title(\"Training error and validation error\")\n",
    "    plt.savefig(\"Dropout {} TVerror.pdf\".format(incl_prob))\n",
    "\n",
    "    result, evaluation = optimiser.eval_test_set(valid_data, 'validation')\n",
    "    prediction = result[-1]\n",
    "    predictions.append(prediction)\n",
    "    print('Validation Error with dropout:    ' + str(evaluation['errorvalidation']))\n",
    "\n",
    "    for i in range(6):\n",
    "        f, axarr = plt.subplots(2, sharex=True, figsize=(17,8))\n",
    "        axarr[0].plot(motion_data[:10000,i], color = 'cadetblue')\n",
    "        axarr[0].set_title('Actual motion data')\n",
    "        axarr[1].plot(prediction[:10000,i], color = 'cadetblue')\n",
    "        axarr[1].set_title('Raw prediction with {} hidden units'.format(hidden_dim))\n",
    "        plt.suptitle(\"Comparison of Actual Motion and Prediction on Dimension {}\".format(i+1), size=20)\n",
    "        plt.savefig(\"Drop {0} ComparisonDim_{1}.pdf\".format(incl_prob, i+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "motion_transpose = np.array(motion_data.transpose())\n",
    "prediction_transpose = np.array(prediction.transpose())\n",
    "\n",
    "pearson_correlation_coefficient = np.corrcoef(motion_transpose, prediction_transpose)\n",
    "plot = sns.heatmap(pearson_correlation_coefficient, center=0, linewidths=.5)\n",
    "fig = plot.get_figure()\n",
    "fig.savefig(\"Dropout Heatmap\")\n",
    "for i in range(6):\n",
    "    print(\"CC of dimension {}\".format(i), \" is \", pearson_correlation_coefficient[i,i+6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Test Batch Norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "motion_data = np.load('data/Wiki/validation_extro.npz')['targets']\n",
    "batch_size = 100  \n",
    "init_scale = 0.01  \n",
    "learning_rate = 0.001  \n",
    "num_epochs = 1 \n",
    "stats_interval = 1 \n",
    "weight_penalty = L2Penalty(1e-5)\n",
    "hidden_dim = 150\n",
    "\n",
    "rng.seed(seed)\n",
    "train_data.reset()\n",
    "valid_data.reset()\n",
    "\n",
    "train_data.batch_size = batch_size \n",
    "valid_data.batch_size = batch_size\n",
    "\n",
    "weights_init = GlorotUniformInit(rng=rng, gain=2.**0.5)\n",
    "biases_init = ConstantInit(0.)\n",
    "\n",
    "model = MultipleLayerModel([\n",
    "    AffineLayer(input_dim, hidden_dim, weights_init, biases_init, weight_penalty),\n",
    "    BatchNormalizationLayer(input_dim = 150),\n",
    "    ReluLayer(),\n",
    "    AffineLayer(hidden_dim, output_dim, weights_init, biases_init, weight_penalty),\n",
    "])\n",
    "\n",
    "error = SumOfSquaredDiffsError()\n",
    "learning_rule = AdamLearningRule(learning_rate=learning_rate)\n",
    "data_monitors={'acc': lambda y, t: (y.argmax(-1) == t.argmax(-1)).mean()}\n",
    "\n",
    "optimiser = Optimiser(\n",
    "        model, error, learning_rule, train_data, valid_data, data_monitors)\n",
    "\n",
    "stats, keys, run_time = optimiser.train(num_epochs=num_epochs, stats_interval=stats_interval)\n",
    "\n",
    "fig_1 = plt.figure(figsize=(8, 4))\n",
    "ax_1 = fig_1.add_subplot(111)\n",
    "# plt.ylim([0.0305, 0.0973])\n",
    "for k in ['error(train)', 'error(valid)']:\n",
    "    ax_1.plot(np.arange(1, stats.shape[0]) * stats_interval, \n",
    "              stats[1:, keys[k]], label=k)\n",
    "ax_1.legend(loc=0)\n",
    "ax_1.set_xlabel('Epoch number')\n",
    "plt.title(\"Training error and validation error\")\n",
    "plt.savefig(\"BN TVerror.pdf\")\n",
    "\n",
    "result, evaluation = optimiser.eval_test_set(valid_data, 'validation')\n",
    "prediction = result[-1]\n",
    "print('Validation Error with dropout:    ' + str(evaluation['errorvalidation']))\n",
    "\n",
    "for i in range(6):\n",
    "    f, axarr = plt.subplots(2, sharex=True, figsize=(17,8))\n",
    "    axarr[0].plot(motion_data[:10000,i], color = 'cadetblue')\n",
    "    axarr[0].set_title('Actual motion data')\n",
    "    axarr[1].plot(prediction[:10000,i], color = 'cadetblue')\n",
    "    axarr[1].set_title('Raw prediction with {} hidden units'.format(hidden_dim))\n",
    "    plt.suptitle(\"Comparison of Actual Motion and Prediction on Dimension {}\".format(i+1), size=20)\n",
    "    plt.savefig(\"BN ComparisonDim_{0}.pdf\".format(i+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "motion_transpose = np.array(motion_data.transpose())\n",
    "prediction_transpose = np.array(prediction.transpose())\n",
    "\n",
    "pearson_correlation_coefficient = np.corrcoef(motion_transpose, prediction_transpose)\n",
    "plot = sns.heatmap(pearson_correlation_coefficient, center=0, linewidths=.5)\n",
    "fig = plot.get_figure()\n",
    "fig.savefig(\"BN Heatmap\")\n",
    "for i in range(6):\n",
    "    print(\"CC of dimension {}\".format(i), \" is \", pearson_correlation_coefficient[i,i+6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Test error/loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "motion_data = np.load('data/Wiki/validation_intro.npz')['targets']\n",
    "batch_size = 100  \n",
    "init_scale = 0.01  \n",
    "learning_rate = 0.001  \n",
    "num_epochs = 10\n",
    "stats_interval = 1 \n",
    "weight_penalty = L2Penalty(1e-5)\n",
    "hidden_dim = 150\n",
    "\n",
    "rng.seed(seed)\n",
    "train_data.reset()\n",
    "valid_data.reset()\n",
    "\n",
    "train_data.batch_size = batch_size \n",
    "valid_data.batch_size = batch_size\n",
    "\n",
    "weights_init = GlorotUniformInit(rng=rng, gain=2.**0.5)\n",
    "biases_init = ConstantInit(0.)\n",
    "\n",
    "model = MultipleLayerModel([\n",
    "    AffineLayer(input_dim, hidden_dim, weights_init, biases_init, weight_penalty),\n",
    "    ReluLayer(),\n",
    "    AffineLayer(hidden_dim, output_dim, weights_init, biases_init, weight_penalty),\n",
    "])\n",
    "\n",
    "error = L1Error()\n",
    "learning_rule = AdamLearningRule(learning_rate=learning_rate)\n",
    "data_monitors={'acc': lambda y, t: (y.argmax(-1) == t.argmax(-1)).mean()}\n",
    "\n",
    "optimiser_L1 = Optimiser(\n",
    "        model, error, learning_rule, train_data, valid_data, data_monitors)\n",
    "\n",
    "stats_L1, keys_L1, run_time_L1 = optimiser_L1.train(num_epochs=num_epochs, stats_interval=stats_interval)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig_1 = plt.figure(figsize=(8, 4))\n",
    "ax_1 = fig_1.add_subplot(111)\n",
    "# plt.ylim([0.0305, 0.0973])\n",
    "for k in ['error(train)', 'error(valid)']:\n",
    "    ax_1.plot(np.arange(1, stats_L1.shape[0]) * stats_interval, \n",
    "              stats_L1[1:, keys_L1[k]], label=k)\n",
    "ax_1.legend(loc=0)\n",
    "ax_1.set_xlabel('Epoch number')\n",
    "plt.title(\"Training error and validation error\")\n",
    "plt.savefig(\"L1 loss TVerror.pdf\")\n",
    "\n",
    "result, evaluation = optimiser_L1.eval_test_set(valid_data, 'validation')\n",
    "prediction_L1 = result[-1]\n",
    "print('Validation Error:    ' + str(evaluation['errorvalidation']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "motion_transpose = np.array(motion_data.transpose())\n",
    "prediction_transpose = np.array(prediction_L1.transpose())\n",
    "\n",
    "pearson_correlation_coefficient = np.corrcoef(motion_transpose, prediction_transpose)\n",
    "plot = sns.heatmap(pearson_correlation_coefficient, center=0, linewidths=.5)\n",
    "fig = plot.get_figure()\n",
    "fig.savefig(\"L1 Heatmap\")\n",
    "for i in range(6):\n",
    "    print(\"CC of dimension {}\".format(i), \" is \", pearson_correlation_coefficient[i,i+6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_size = 100  \n",
    "init_scale = 0.01  \n",
    "learning_rate = 0.001  \n",
    "num_epochs = 30\n",
    "stats_interval = 1 \n",
    "weight_penalty = L2Penalty(1e-5)\n",
    "hidden_dim = 150\n",
    "\n",
    "rng.seed(seed)\n",
    "train_data.reset()\n",
    "valid_data.reset()\n",
    "\n",
    "train_data.batch_size = batch_size \n",
    "valid_data.batch_size = batch_size\n",
    "\n",
    "weights_init = GlorotUniformInit(rng=rng, gain=2.**0.5)\n",
    "biases_init = ConstantInit(0.)\n",
    "\n",
    "model = MultipleLayerModel([\n",
    "    AffineLayer(input_dim, hidden_dim, weights_init, biases_init, weight_penalty),\n",
    "    ReluLayer(),\n",
    "    AffineLayer(hidden_dim, output_dim, weights_init, biases_init, weight_penalty),\n",
    "])\n",
    "\n",
    "error = SumOfSquaredDiffsError()\n",
    "learning_rule = AdamLearningRule(learning_rate=learning_rate)\n",
    "data_monitors={'acc': lambda y, t: (y.argmax(-1) == t.argmax(-1)).mean()}\n",
    "\n",
    "optimiser_L2 = Optimiser(\n",
    "        model, error, learning_rule, train_data, valid_data, data_monitors)\n",
    "\n",
    "stats_L2, keys_L2, run_time_L2 = optimiser_L2.train(num_epochs=num_epochs, stats_interval=stats_interval)\n",
    "\n",
    "fig_1 = plt.figure(figsize=(8, 4))\n",
    "ax_1 = fig_1.add_subplot(111)\n",
    "# plt.ylim([0.0305, 0.0973])\n",
    "for k in ['error(train)', 'error(valid)']:\n",
    "    ax_1.plot(np.arange(1, stats_L2.shape[0]) * stats_interval, \n",
    "              stats_L2[1:, keys_L2[k]], label=k)\n",
    "ax_1.legend(loc=0)\n",
    "ax_1.set_xlabel('Epoch number')\n",
    "plt.title(\"Training error and validation error\")\n",
    "plt.savefig(\"L2 loss TVerror.pdf\")\n",
    "\n",
    "result, evaluation = optimiser_L2.eval_test_set(valid_data, 'validation')\n",
    "prediction_L2 = result[-1]\n",
    "print('Validation Error with dropout:    ' + str(evaluation['errorvalidation']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "motion_transpose = np.array(motion_data.transpose())\n",
    "prediction_transpose = np.array(prediction_L2.transpose())\n",
    "\n",
    "pearson_correlation_coefficient = np.corrcoef(motion_transpose, prediction_transpose)\n",
    "plot = sns.heatmap(pearson_correlation_coefficient, center=0, linewidths=.5)\n",
    "fig = plot.get_figure()\n",
    "fig.savefig(\"L2 Heatmap\")\n",
    "for i in range(6):\n",
    "    print(\"CC of dimension {}\".format(i), \" is \", pearson_correlation_coefficient[i,i+6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(6):\n",
    "    f, axarr = plt.subplots(3, sharex=True, sharey=True, figsize=(15,8))\n",
    "    axarr[0].plot(motion_data[:10000,i], color = 'cadetblue')\n",
    "    axarr[0].set_title('Actual motion data')\n",
    "    axarr[1].plot(prediction_L1[:10000,i], color = 'cadetblue')\n",
    "    axarr[1].set_title('Raw prediction with L1 at epoch 10'.format(hidden_dim))\n",
    "    axarr[2].plot(prediction_L2[:10000,i], color = 'cadetblue')\n",
    "    axarr[2].set_title('Raw prediction with L2 at epoch 30'.format(hidden_dim))    \n",
    "    plt.suptitle(\"Comparison of Actual Motion and Prediction on Dimension {}\".format(i+1), size=20)\n",
    "    plt.savefig(\"loss ComparisonDim_{0}.pdf\".format(i+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a function that smooths the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def smooth_prediction(raw_prediction):\n",
    "    output_shape = raw_prediction.shape\n",
    "    # make a matrix that adds 20 lines paddings at the beginning & end of raw prediction\n",
    "    calculation_frame = np.zeros((output_shape[0]+40, output_shape[1]))\n",
    "    output = np.zeros(raw_prediction.shape)\n",
    "    calculation_frame[20:-20, :] = raw_prediction\n",
    "\n",
    "    for i in range(output_shape[0]):\n",
    "        output[i,:] = calculation_frame[i+20,:] + \\\n",
    "        0.9 * (calculation_frame[i+21,:] + calculation_frame[i+19,:]) + \\\n",
    "        0.9 * (calculation_frame[i+22,:] + calculation_frame[i+18,:]) + \\\n",
    "        0.8 * (calculation_frame[i+23,:] + calculation_frame[i+17,:]) + \\\n",
    "        0.8 * (calculation_frame[i+24,:] + calculation_frame[i+16,:]) + \\\n",
    "        0.7 * (calculation_frame[i+25,:] + calculation_frame[i+15,:]) + \\\n",
    "        0.7 * (calculation_frame[i+26,:] + calculation_frame[i+14,:]) + \\\n",
    "        0.6 * (calculation_frame[i+27,:] + calculation_frame[i+13,:]) + \\\n",
    "        0.6 * (calculation_frame[i+28,:] + calculation_frame[i+12,:]) + \\\n",
    "        0.5 * (calculation_frame[i+29,:] + calculation_frame[i+11,:]) + \\\n",
    "        0.5 * (calculation_frame[i+30,:] + calculation_frame[i+10,:]) + \\\n",
    "        0.4 * (calculation_frame[i+31,:] + calculation_frame[i+9,:]) + \\\n",
    "        0.4 * (calculation_frame[i+32,:] + calculation_frame[i+8,:]) + \\\n",
    "        0.3 * (calculation_frame[i+33,:] + calculation_frame[i+7,:]) + \\\n",
    "        0.3 * (calculation_frame[i+34,:] + calculation_frame[i+6,:]) + \\\n",
    "        0.2 * (calculation_frame[i+35,:] + calculation_frame[i+5,:]) + \\\n",
    "        0.2 * (calculation_frame[i+36,:] + calculation_frame[i+4,:]) + \\\n",
    "        0.1 * (calculation_frame[i+37,:] + calculation_frame[i+3,:]) + \\\n",
    "        0.1 * (calculation_frame[i+38,:] + calculation_frame[i+2,:]) + \\\n",
    "        0.1 * (calculation_frame[i+39,:] + calculation_frame[i+1,:])\n",
    "    output /= [2.0, 2.0, 2.0, 3.0, 2.0, 2.0]\n",
    "\n",
    "    # Adjust motion to origin\n",
    "#     motion_mean = np.mean(output,axis=0)\n",
    "#     output = output - motion_mean\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dct_smoothing(raw_prediction, frequency_factor, window_size):\n",
    "    trans = raw_prediction.transpose()\n",
    "    output = np.zeros(trans.shape)\n",
    "    \n",
    "    window_number = (output.shape[1] + window_size - 1) // window_size\n",
    "    temp_output = np.zeros((trans.shape[0], window_size*window_number))\n",
    "    \n",
    "    # Do dct smoothing on every window individually\n",
    "    for i in range(trans.shape[0]):\n",
    "        motion = np.zeros(window_size * window_number)\n",
    "        motion[:trans.shape[1]] = trans[i,:]\n",
    "        for j in range(window_number):\n",
    "            temp = np.zeros(window_size)\n",
    "            temp_freq = np.zeros(window_size)\n",
    "            temp_freq[:frequency_factor] = dct(motion[j*window_size:(j+1)*window_size], norm=\"ortho\")[:frequency_factor]\n",
    "            temp_output[i, j*window_size:(j+1)*window_size] = dct(temp_freq, 3, norm=\"ortho\")\n",
    "            #ith_dim_freq = np.zeros(trans.shape[1])\n",
    "            #ith_dim_freq[:frequency_factor] = dct(trans[i], norm=\"ortho\")[:frequency_factor]\n",
    "            #output[i] = dct(ith_dim_freq, 3, norm=\"ortho\")\n",
    "    \n",
    "    output[:, :raw_prediction.shape[0]] = temp_output[:, :raw_prediction.shape[0]]  \n",
    "    output = output.transpose()\n",
    "    \n",
    "    smooth_boundary = output.copy()\n",
    "    \n",
    "    # Smooth the boundaries\n",
    "#     for i in range(1, window_number):\n",
    "#         mid_left = i * window_size-1\n",
    "#         mid_right = mid_left + 1\n",
    "#         start = mid_left - 29\n",
    "#         end = mid_right + 29\n",
    "        \n",
    "#         mid_mean = (output[mid_left] + output[mid_right]) / 2.0\n",
    "        \n",
    "#         for j in range(31):\n",
    "#             smooth_boundary[start + j,:] = (1 - j / 30.0) * output[j] + j / 30.0 * mid_mean\n",
    "#             smooth_boundary[end - j,:] = (1 - j / 30.0) * output[j] + j / 30.0 * mid_mean\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Produce output of test sets using L1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Loop over 1-6 test cases\n",
    "\n",
    "for i in range(1,7):\n",
    "    test_data = HMS300dDataProvider('test{0}'.format(i),'extro', 'Wiki' rng=rng)\n",
    "    result, evaluation = optimiser_L1.eval_test_set(test_data, 'test')\n",
    "    print('Error:    ' + str(evaluation['errortest']))\n",
    "    \n",
    "    time_intervals = np.loadtxt(\"ExtrovertRawData/Words/{0}\".format(i), usecols=range(4, 6), dtype=\"int\")\n",
    "    prediction = np.zeros((time_intervals[-1,1] + 300, 6))\n",
    "    counter = 0\n",
    "    for line in time_intervals:\n",
    "        for j in range(line[0], line[1]):\n",
    "            prediction[j] = result[-1][counter]\n",
    "            counter += 1\n",
    "    prediction_smooth = smooth_prediction(prediction)\n",
    "\n",
    "    np.savetxt('Predictions/300dL1/fake_extro_L1_{0}.txt'.format(i), prediction, fmt=\"%.7f\")\n",
    "    np.savetxt('Predictions/300dL1/fake_extro_L1_smooth_{0}.txt'.format(i), prediction_smooth, fmt=\"%.7f\")\n",
    "    \n",
    "    motion_data = np.loadtxt(\"ExtrovertRawData/Motion/{0}.rov\".format(i), skiprows=17, usecols=range(0, 6))\n",
    "    print(\"Motion length: \", motion_data.shape[0])\n",
    "\n",
    "    # Two subplots, the axes array is 1-d\n",
    "    f, axarr = plt.subplots(3, sharex=True, sharey=True, figsize=(17,8))\n",
    "    axarr[0].plot(motion_data[:10000,i-1], color = 'cadetblue')\n",
    "    axarr[0].set_title('Actual motion data')\n",
    "    axarr[1].plot(prediction[:10000,i-1], color = 'cadetblue')\n",
    "    axarr[1].set_title('Raw prediction')\n",
    "    axarr[2].plot(prediction_smooth[:10000,i-1], color = 'cadetblue')\n",
    "    axarr[2].set_title('Smoothed prediction')\n",
    "    plt.suptitle('Test case {0}, comparison on dimention {1}'.format(i, i), size = 20)\n",
    "    plt.savefig('Predictions/300dL1/L1 fake_extro case {0} dim {1}.pdf'.format(i, i))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Produce output of test sets using L2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Loop over 1-6 test cases\n",
    "\n",
    "for i in range(1,7):\n",
    "    test_data = HMS300dDataProvider('test{0}'.format(i), rng=rng)\n",
    "    result, evaluation = optimiser_L1.eval_test_set(test_data, 'test')\n",
    "    print('Error:    ' + str(evaluation['errortest']))\n",
    "    \n",
    "    time_intervals = np.loadtxt(\"ExtrovertRawData/Words/{0}\".format(i), usecols=range(4, 6), dtype=\"int\")\n",
    "    prediction = np.zeros((time_intervals[-1,1] + 300, 6))\n",
    "    counter = 0\n",
    "    for line in time_intervals:\n",
    "        for j in range(line[0], line[1]):\n",
    "            prediction[j] = result[-1][counter]\n",
    "            counter += 1\n",
    "    prediction_smooth = smooth_prediction(prediction)\n",
    "\n",
    "    np.savetxt('Predictions/300dL2/fake_extro_L2_{0}.txt'.format(i), prediction, fmt=\"%.7f\")\n",
    "    np.savetxt('Predictions/300dL2/fake_extro_L2_smooth_{0}.txt'.format(i), prediction_smooth, fmt=\"%.7f\")\n",
    "    \n",
    "    motion_data = np.loadtxt(\"ExtrovertRawData/Motion/{0}.rov\".format(i), skiprows=17, usecols=range(0, 6))\n",
    "    print(\"Motion length: \", motion_data.shape[0])\n",
    "\n",
    "    # Two subplots, the axes array is 1-d\n",
    "    f, axarr = plt.subplots(3, sharex=True, sharey=True, figsize=(17,8))\n",
    "    axarr[0].plot(motion_data[:10000,i-1], color = 'cadetblue')\n",
    "    axarr[0].set_title('Actual motion data')\n",
    "    axarr[1].plot(prediction[:10000,i-1], color = 'cadetblue')\n",
    "    axarr[1].set_title('Raw prediction')\n",
    "    axarr[2].plot(prediction_smooth[:10000,i-1], color = 'cadetblue')\n",
    "    axarr[2].set_title('Smoothed prediction')\n",
    "    plt.suptitle('Test case {0}, comparison on dimention {1}'.format(i, i), size = 20)\n",
    "    plt.savefig('Predictions/300dL2/L2 fake_extro case {0} dim {1}.pdf'.format(i, i))\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
